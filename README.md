---
title: Recipe LM API
emoji: ğŸ³
colorFrom: yellow
colorTo: red
sdk: docker
pinned: false
startup_duration_timeout: 300
---

# recipe-lm

An end-to-end recipe generation system: a [Dagster](https://dagster.io/) training pipeline that fine-tunes [Gemma-2B](https://huggingface.co/google/gemma-2b) on [recipe data](https://huggingface.co/datasets/corbt/all-recipes) with LoRA, a FastAPI streaming server, and a React web UI ([kitchen-genie](https://github.com/glee2429/kitchen-genie)).

The trained adapter is published at [ClaireLee2429/gemma-2b-recipes-lora](https://huggingface.co/ClaireLee2429/gemma-2b-recipes-lora).

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Training Pipeline                       â”‚
â”‚                                                              â”‚
â”‚  HuggingFace Hub â”€â”€â–º Dagster Pipeline â”€â”€â–º LoRA Adapter       â”‚
â”‚  (corbt/all-recipes)  (clean, tokenize,   (pushed to HF Hub) â”‚
â”‚                        split, train)                         â”‚
â”‚                                                              â”‚
â”‚  Alternative: Google Colab + Unsloth (2x faster on T4 GPU)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Inference & Serving                     â”‚
â”‚                                                              â”‚
â”‚  inference.py â”€â”€ CLI script for batch generation             â”‚
â”‚  server.py â”€â”€â”€â”€â”€ FastAPI + SSE streaming (POST /generate)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Web Frontend                          â”‚
â”‚                                                              â”‚
â”‚  kitchen-genie â”€â”€ React + Vite + Tailwind + shadcn/ui        â”‚
â”‚                   Streams tokens from server.py in real time  â”‚
â”‚                   github.com/glee2429/kitchen-genie          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

```bash
# Install and start the API server
pip install -e ".[serve]"
huggingface-cli login
python server.py --adapter ClaireLee2429/gemma-2b-recipes-lora

# Start the frontend (in a separate terminal)
cd ../kitchen-genie && npm install && npm run dev

# Open http://localhost:8080
```

## Project Structure

```
recipe-lm/
â”œâ”€â”€ data_pipeline/          # Dagster training pipeline
â”‚   â”œâ”€â”€ assets/             #   download, clean, tokenize, split, train
â”‚   â”œâ”€â”€ resources.py        #   HuggingFaceConfig resource
â”‚   â”œâ”€â”€ io_managers.py      #   Arrow dataset I/O manager
â”‚   â””â”€â”€ definitions.py      #   Dagster definitions
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ train_unsloth.ipynb # Colab notebook (Unsloth + T4 GPU)
â”œâ”€â”€ inference.py            # CLI inference with post-processing
â”œâ”€â”€ server.py               # FastAPI streaming API server
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml        # Default pipeline configuration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_assets.py      # Pipeline asset tests
â””â”€â”€ pyproject.toml          # Dependencies (dev, colab, serve)
```

## Training

Run the Dagster pipeline locally or use the Colab notebook for free GPU training:

```bash
# Local (Dagster)
pip install -e ".[dev]"
dagster dev

# Colab (Unsloth, ~2x faster)
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/glee2429/recipe-lm/blob/main/notebooks/train_unsloth.ipynb)

Pipeline: `raw_dataset â†’ cleaned_dataset â†’ tokenized_dataset â†’ train_val_splits â†’ trained_model`

See `configs/default.yaml` for all training parameters (LoRA rank, learning rate, batch size, etc.).

## Inference

```bash
# CLI
python inference.py --prompt "Recipe for chocolate chip cookies:"
python inference.py --adapter ClaireLee2429/gemma-2b-recipes-lora --prompt "Recipe for soup:"
python inference.py --no-adapter --prompt "Recipe for tacos:"  # base model comparison

# API server
python server.py --adapter ClaireLee2429/gemma-2b-recipes-lora
curl -N -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Recipe for pasta carbonara:"}'
```

Run `python inference.py --help` or `python server.py --help` for all options.

## Tests

```bash
pytest tests/
```
